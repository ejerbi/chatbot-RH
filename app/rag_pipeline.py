import streamlit as st
from PyPDF2 import PdfReader
import faiss
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from transformers import BartForConditionalGeneration, BartTokenizer
import re
import os

torch.set_num_threads(1)

# ---- 1. Extraction et nettoyage du texte du PDF ----
def load_documents(pdf_path):
    reader = PdfReader(pdf_path)
    documents = []
    for page in reader.pages:
        text = page.extract_text()
        if text:
            text = re.sub(r'\s+', ' ', text).strip()  # Supprimer les espaces inutiles
            documents.append(text)
    return documents

# ---- 2. S√©paration des questions-r√©ponses ----
def split_documents(documents):
    chunks = []
    for document in documents:
        questions_reponses = re.split(r"\bQ\s*:\s*", document)
        for qr in questions_reponses:
            qr = qr.strip()
            if "R :" in qr:
                qr = "Q : " + qr  
                chunks.append(qr)
    return chunks

# ---- 3. Stockage des embeddings avec FAISS ----
def store_embeddings(chunks):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(chunks, convert_to_tensor=True)

    embeddings_np = embeddings.cpu().numpy()
    faiss_index = faiss.IndexFlatL2(embeddings_np.shape[1])
    faiss_index.add(embeddings_np)

    return faiss_index, embeddings_np, model

# ---- 4. Recherche des documents pertinents ----
def retrieve_relevant_documents(query, faiss_index, chunks, embedding_model):
    query_embedding = embedding_model.encode([query])

    results = faiss_index.search(query_embedding, k=3)  # R√©cup√©rer les 3 meilleurs r√©sultats
    relevant_chunks = [chunks[i] for i in results[1].flatten()]

    return relevant_chunks if relevant_chunks else ["D√©sol√©, je n'ai pas trouv√© de r√©ponse pertinente."]

# ---- 5. G√©n√©ration de la r√©ponse avec BART ----
def generate_response(relevant_chunks, bart_model, bart_tokenizer):
    context = " ".join(relevant_chunks)
    context = re.sub(r'\s+', ' ', context).strip()

    inputs = bart_tokenizer(context, return_tensors="pt", truncation=True, padding=True, max_length=1024)
    
    summary_ids = bart_model.generate(
        inputs['input_ids'], 
        max_length=150, 
        num_beams=4, 
        no_repeat_ngram_size=2, 
        do_sample=True, 
        temperature=0.7
    )
    
    response = bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return response

# ---- 6. Chargement du mod√®le BART une seule fois ----
model_name = "facebook/bart-large-cnn"
bart_model = BartForConditionalGeneration.from_pretrained(model_name)
bart_tokenizer = BartTokenizer.from_pretrained(model_name)

# ---- 7. Interface Streamlit ----
st.title("ü§ñ Chatbot FAQ - RH (avec chemin d'acc√®s)")

# ---- Champ pour entrer le chemin du fichier PDF ----
pdf_path = "../data/FAQ_RH.pdf"  # Chemin par d√©faut
if pdf_path:
    if os.path.exists(pdf_path):
        st.spinner("üìñ Lecture du fichier...")
        documents = load_documents(pdf_path)
        chunks = split_documents(documents)
        faiss_index, embeddings, embedding_model = store_embeddings(chunks)
        st.session_state["faiss_index"] = faiss_index
        st.session_state["chunks"] = chunks
        st.session_state["embedding_model"] = embedding_model
        st.success("‚úÖ Fichier charg√© avec succ√®s ! Posez votre question.")
    else:
        st.error("‚ùå Chemin invalide ! Veuillez v√©rifier l'emplacement du fichier.")

# ---- Gestion de l'historique des messages ----
if "messages" not in st.session_state:
    st.session_state.messages = []

# ---- Affichage de l'historique ----
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ---- Zone de saisie utilisateur ----
query = st.chat_input("Posez une question ici...")

if query and "faiss_index" in st.session_state:
    with st.chat_message("user"):
        st.markdown(query)
    st.session_state.messages.append({"role": "user", "content": query})

    # ---- R√©cup√©ration et g√©n√©ration de la r√©ponse ----
    with st.spinner("üí° Recherche de la r√©ponse..."):
        relevant_chunks = retrieve_relevant_documents(query, 
                                                      st.session_state["faiss_index"], 
                                                      st.session_state["chunks"], 
                                                      st.session_state["embedding_model"])
        response = generate_response(relevant_chunks, bart_model, bart_tokenizer)

    # ---- Affichage de la r√©ponse ----
    with st.chat_message("assistant"):
        st.markdown(response)
    st.session_state.messages.append({"role": "assistant", "content": response})
